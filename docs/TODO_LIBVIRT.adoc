= Improvements and corrections for Libvirt as infra

The following isn't a decision of things to do, just a rather random list of things that might (or might not) end in future version of the Libvirt integration. It is just easier to document those things as I work on the topic, rather than creating individual issues that might never be addressed.

== Make the VM provisioning purely local

It seems we can avoid spinning a web server to provide the ISO content by using a command line like the following:

------------------------------------------------------------------------
virt-install \
  --name rhel7.5-Ceph-test-singlenode \
  --os-variant rhel7 \
  --initrd-inject=/home/pcfe/work/git/HouseNet/kickstart/RHEL75-Ceph-singlenode-ks.cfg \
  --location /mnt/ISO_images/rhel-server-7.5-x86_64-dvd.iso \
  --extra-args "ks=file:/RHEL75-Ceph-singlenode-ks.cfg console=ttyS0,115200" \
  --ram 2048 \
  --disk pool=default,boot_order=1,format=qcow2,bus=virtio,discard=unmap,sparse=yes,size=10 \
  --disk pool=default,boot_order=2,format=qcow2,bus=virtio,discard=unmap,sparse=yes,size=5 \
  --disk pool=default,boot_order=3,format=qcow2,bus=virtio,discard=unmap,sparse=yes,size=5 \
  --disk pool=default,boot_order=4,format=qcow2,bus=virtio,discard=unmap,sparse=yes,size=5 \
  --controller scsi,model=virtio-scsi \
  --rng /dev/random \
  --boot useserial=on \
  --vcpus 1 \
  --cpu host \
  --nographics \
  --accelerate \
  --network network=default,model=virtio
------------------------------------------------------------------------

This would probably be a less intrusive and resource intensive approach.

== Make the dynamic inventory script more flexible

Could be useful to make it easier to add/remove nodes without configuring multiple places in the (static) inventory. Two ideas:

- either add metadata to the created VMs (but how does it work exactly?).
- or use the description field to pack information, e.g. using an ini or json format (quoting might be an issue here).

== Split the inventory in infra and cluster inventories

I'm thinking that it would be much more flexible and easier to maintain to split the inventory in a "cluster type" and an "infra type" and combine them with multiple  -i options, .e.g -i libvirt_inv/ -i 3_nodes_cluster_inv/.

Just an idea at this stage and I'm not sure it's easily possible to get the expected flexiblity, but with the right dynamic script, it might be feasible.

== Make the Libvirt inventory more robust

. the inventory shouldn't fail if title or description is missing

== Improve the playbooks / roles using ideas from others

Following sources have been identified and could be used:

- https://github.com/nmajorov/libvirt-okd
- https://docs.google.com/document/d/1Mbd2v6j3AQlbiY_zbZF5fWWwXvenQf8EDw7oW5907Hs/edit?usp=drivesdk

== Improve the CONTRIBUTE_PROVISIONER.md

Just taking notes for future improvements as I learn myself through casl, feel free to already review, I'll add them to the already document once I'm finished here:

NOTE: `{provisioner}` and `{PROVISIONER}` stand for your provisioner (e.g. libvirt etc.) either in capitals or not.

- create following directories in the `casl-ansible` repository:
* `playbooks/openshift/{provisioner}`
* `inventory/sample.{provisioner}.example.com.d/inventory` (and optionally `files` or others)
- create a playbook which creates VMs from your provisioner as `playbooks/openshift/{provisioner}/provision.yml` and make sure it is called from `playbooks/openshift/provision-instances.yml` based on the variable `hosting_infrastructure` set to your provisioner.
* re-use as much as possible roles from the `infra-ansible` repo or add new generic roles that support your infrastructure provider independently from `casl-ansible`.
- create a sample inventory respecting following requirements:
* it respects the usual OpenShift inventory settings and makes sure that the nodes created during the provisioning phase are neatly put into the right groups.
* this most probably pre-requisites that there is a dynamic inventory script created, which pulls the information about the VMs from the provisioner. This script is created into `inventory/scripts/` and linked into `inventory/sample.{provisioner}.example.com.d/inventory` (FIXME: why this complexity?). Your script must especially make sure that `ansible_host` is defined so that the Ansible connection isn't relying on the name in the inventory, which is recommended to be independent from the provisioning.
* The following groups are foreseen:
** `cluster_hosts` has `OSEv3` as sole children and holds the non-OpenShift-specific variables
** `seed-hosts` ???
* it defines following variables required during the next steps of the end-to-end process:
** `hosting_infrastructure` set to `{provisioner}`.
* following variables are optional:
** `docker_storage_block_device` e.g. `/dev/vdb` (implying that each created VM has two disks, one being reserved for Docker???).
** `openshift_authorized_key_url` or `openshift_authorized_key_file` shall contain a URL or a path to the SSH key authorized for the Ansible user.
- create a document `docs/PROVISIONING_{PROVISIONER}.adoc` explaining how to adapt and use your provisioner. Some notes about the considerations you've made during implementation is surely not a bad idea.

== Perhaps fix inventory to avoid unwanted actions

The installer executed 1 action on cloud-host.local and that doesn't seem right but no clue which one (it was marked OK). It would also be good to review the 22 OK-actions on localhost.

== Fix warnings and errors in oc adm diagnostics

Quite verbose and I don't have quite enough experience to decide which errors are relevant and which not:

- `E0121 12:41:32.229577   80499 helpers.go:134] Encountered config error json: unknown field "masterCount" in object *config.MasterConfig, raw JSON:`
- errors about AggregatedLogging, Kibana, etc...
- many notes/infos about extra permissions
- error about missing iptables:
+
------------------------------------------------------------------------
ERROR: [DS3002 from diagnostic UnitStatus@openshift/origin/pkg/oc/cli/admin/diagnostics/diagnostics/systemd/unit_status.go:59]
       systemd unit atomic-openshift-node depends on unit iptables, which is not loaded.

       iptables is used by nodes for container networking.
       Connections to a container will fail without it.
       An administrator probably needs to install the iptables unit with:

         # yum install iptables

       If it is already installed, you may need to reload the definition with:

         # systemctl reload iptables
------------------------------------------------------------------------

== Automate network creation in Libvirt

Shouldn't be a big deal using the virt-net module of Ansible (requires python2/3-lxml).

It would also make sense to replace the XML file with an XML template to get some more flexibility.

== Automate the dnsmasq service for the apps wildcard domain

------------------------------------------------------------------------
ip link add dummy0 type dummy
ip address add 192.168.123.254 dev dummy0
ip address show dev dummy0 up
dnsmasq --interface=dummy0 --no-daemon --log-queries=extra --bind-interfaces --clear-on-reload --address=/apps.local/192.168.123.64
------------------------------------------------------------------------
